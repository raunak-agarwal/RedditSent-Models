{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/NLPrinceton/SARC.git\n",
    "# !git clone https://github.com/NLPrinceton/text_embedding.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently working on the balanced section for political comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementing the Baseline (amazon glove 1600d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir SARC/pol\n",
    "%cd SARC/pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.cs.princeton.edu/SARC/2.0/pol/test-balanced.csv.bz2\n",
    "!wget http://nlp.cs.princeton.edu/SARC/2.0/pol/train-balanced.csv.bz2\n",
    "!wget http://nlp.cs.princeton.edu/SARC/2.0/pol/comments.json.bz2\n",
    "!bzip2 -d *.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARC\t\t\t    sarc-build.ipynb\r\n",
      "Untitled.ipynb\t\t    sarc-implement-Copy1.ipynb\r\n",
      "__init__.py\t\t    sarc-implement.ipynb\r\n",
      "__pycache__\t\t    sarutils.py\r\n",
      "bert\t\t\t    temp\r\n",
      "core\t\t\t    test\r\n",
      "flair-benchmark-prepare.py  test-balanced.csv\r\n",
      "model-test.py\t\t    test-unbalanced.csv\r\n",
      "models\t\t\t    text_embedding\r\n",
      "r.py\t\t\t    train-balanced-sarc.csv.gz\r\n",
      "rhp.py\t\t\t    train-balanced-sarcasm.csv\r\n",
      "run-flair.py\t\t    twitter-multi\r\n",
      "sar.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/nlp/reddit/sarcasm/test\n"
     ]
    }
   ],
   "source": [
    "# !mkdir test\n",
    "%cd test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import argparse\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogitCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from text_embedding.features import *\n",
    "from text_embedding.vectors import *\n",
    "from SARC.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sarc_responses(train_file, test_file, comment_file, lower=True):\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    with open(comment_file, 'r') as f:\n",
    "        comments = json.load(f)\n",
    "    train_docs = {'ancestors': [], 'responses': []}\n",
    "    train_ids = {'ancestors':[], 'responses':[]}\n",
    "    train_labels = []\n",
    "    with open(train_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='|')\n",
    "        for row in reader:\n",
    "            ancestors = row[0].split(' ')\n",
    "            responses = row[1].split(' ')\n",
    "            train_ids['ancestors'].append([r for r in ancestors])\n",
    "            train_ids['responses'].append([r for r in responses])\n",
    "            labels = row[2].split(' ')\n",
    "            if lower:\n",
    "                train_docs['ancestors'].append([comments[r]['text'].lower() for r in ancestors])\n",
    "                train_docs['responses'].append([comments[r]['text'].lower() for r in responses])\n",
    "            else:\n",
    "                train_docs['ancestors'].append([comments[r]['text'] for r in ancestors])\n",
    "                train_docs['responses'].append([comments[r]['text'] for r in responses])\n",
    "            train_labels.append(labels)\n",
    "            \n",
    "        \n",
    "    test_docs = {'ancestors': [], 'responses': []}\n",
    "    test_labels = []\n",
    "    test_ids = {'ancestors':[], 'responses':[]}\n",
    "    with open(test_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='|')\n",
    "        for row in reader:\n",
    "            ancestors = row[0].split(' ')\n",
    "            responses = row[1].split(' ')\n",
    "            test_ids['ancestors'].append([r for r in ancestors])\n",
    "            test_ids['responses'].append([r for r in responses])\n",
    "            labels = row[2].split(' ')\n",
    "            if lower:\n",
    "                test_docs['ancestors'].append([comments[r]['text'].lower() for r in ancestors])\n",
    "                test_docs['responses'].append([comments[r]['text'].lower() for r in responses])\n",
    "            else:\n",
    "                test_docs['ancestors'].append([comments[r]['text'] for r in ancestors])\n",
    "                test_docs['responses'].append([comments[r]['text'] for r in responses])\n",
    "            test_labels.append(labels)\n",
    "\n",
    "    return train_docs, test_docs, train_labels, test_labels, train_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse():\n",
    "    d = dict()\n",
    "    d['dataset'] = 'pol'\n",
    "    d['embedding'] = 'SARC/amazon_glove1600.txt'\n",
    "    d['lower'] = True\n",
    "    d['embed'] = True\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments.json  test-balanced.csv  train-balanced.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls SARC/pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.1G\r\n",
      "-rwxr-xr-x 1 ragarwal users 1.1K Jan 29 17:12 LICENSE\r\n",
      "-rwxr-xr-x 1 ragarwal users 1.5K Jan 29 17:12 README.md\r\n",
      "-rwxr-xr-x 1 ragarwal users   33 Jan 29 17:12 __init__.py\r\n",
      "drwxr-xr-x 2 ragarwal users 4.0K Jan 29 17:17 __pycache__\r\n",
      "-rw-r--r-- 1 ragarwal users 8.1G Feb 14  2018 amazon_glove1600.txt\r\n",
      "-rwxr-xr-x 1 ragarwal users 5.1K Jan 29 17:12 eval.py\r\n",
      "drwxr-xr-x 2 ragarwal users 4.0K Jan 29 17:16 pol\r\n",
      "-rwxr-xr-x 1 ragarwal users 2.2K Jan 29 17:12 utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh SARC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load SARC data\n"
     ]
    }
   ],
   "source": [
    "args = parse()\n",
    "\n",
    "SARC = 'SARC/'\n",
    "SARC_POL = SARC + 'pol/'\n",
    "SARC_MAIN = SARC + 'main/'\n",
    "if args['dataset'].lower() == 'pol':\n",
    "    SARC = SARC_POL\n",
    "elif args['dataset'].lower() == 'main':\n",
    "    SARC = SARC_MAIN\n",
    "\n",
    "train_file = SARC+'train-balanced.csv'\n",
    "test_file = SARC+'test-balanced.csv'\n",
    "comment_file = SARC+'comments.json'\n",
    "\n",
    "  # Load SARC pol/main sequences with labels.\n",
    "print('Load SARC data')\n",
    "train_seqs, test_seqs, train_labels, test_labels, train_ids, test_ids = load_sarc_responses(\n",
    "    train_file, test_file, comment_file, lower=args['lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use responses for this method. Ignore ancestors.\n",
    "train_resp = train_seqs['responses']\n",
    "test_resp = test_seqs['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into first and second responses and their labels.\n",
    "# {0: list_of_first_responses, 1: list_of_second_responses}\n",
    "train_docs = {i: [l[i] for l in train_resp] for i in range(2)}\n",
    "test_docs = {i: [l[i] for l in test_resp] for i in range(2)}\n",
    "train_labels = {i: [2*int(l[i])-1 for l in train_labels] for i in range(2)}\n",
    "test_labels = {i: [2*int(l[i])-1 for l in test_labels] for i in range(2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier on all responses in training data. We will later use this\n",
    "# classifier to determine for every sequence which of the 2 responses is more sarcastic.\n",
    "train_all_docs_tok = tokenize(train_docs[0] + train_docs[1])\n",
    "test_all_docs_tok = tokenize(test_docs[0] + test_docs[1])\n",
    "train_all_labels = np.array(train_labels[0] + train_labels[1])\n",
    "test_all_labels = np.array(test_labels[0] + test_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bongs or embeddings.\n",
    "if args['embed']:\n",
    "    print('Create embeddings')\n",
    "    weights = None\n",
    "#     if args.weights == 'sif':\n",
    "#         weights = sif_weights(train_all_docs_tok, 1E-3)\n",
    "#     if args.weights == 'snif':\n",
    "#         weights = sif_weights(train_all_docs_tok, 1E-3)\n",
    "#         weights = {f: 1-w for f, w in weights.items()}\n",
    "    w2v = vocab2vecs({word for doc in train_all_docs_tok+test_all_docs_tok for word in doc}, vectorfile=args['embedding'])\n",
    "    train_all_vecs = docs2vecs(train_all_docs_tok, f2v=w2v, weights=weights)\n",
    "    test_all_vecs = docs2vecs(test_all_docs_tok, f2v=w2v, weights=weights)\n",
    "\n",
    "print('Dimension of representation: %d'%train_all_vecs.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluate the classifier on all responses')\n",
    "clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*train_all_vecs.shape), solver='liblinear', n_jobs=-1, random_state=0) \n",
    "clf.fit(train_all_vecs, train_all_labels)\n",
    "print('\\tTrain acc: ', clf.score(train_all_vecs, train_all_labels))\n",
    "print('\\tTest acc: ', clf.score(test_all_vecs, test_all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing Subword Features with Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_en = BPEmb(lang=\"en\", dim=100, vs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=bpemb_en.encode,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train_docs[0]+train_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_docs[0]+test_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x98084 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the classifier on all responses\n",
      "\tTrain acc:  0.8938396254023998\n",
      "\tTest acc:  0.7025836758661186\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate the classifier on all responses')\n",
    "clf = LogitCV(Cs=[10**i for i in range(-2, 3)], fit_intercept=False, cv=2, dual=np.less(*X_train.shape), solver='liblinear', n_jobs=-1, random_state=0) \n",
    "clf.fit(X_train, train_all_labels)\n",
    "print('\\tTrain acc: ', clf.score(X_train, train_all_labels))\n",
    "print('\\tTest acc: ', clf.score(X_test, test_all_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
